# AudioGuIA

O AudioGuia √© um projeto para Video Captioning, ou seja um sistema de projetado para gerar descri√ß√µes em texto a partir do conte√∫do visual de um v√≠deo. Esse modelo utiliza Long Short-Term Memory (LSTMs), combinando t√©cnicas de vis√£o computacional para extrair informa√ß√µes visuais e processamento de linguagem natural para produzir legendas coerentes e descritivas.

Os benef√≠cios do video captioning s√£o diversos, incluindo a acessibilidade para pessoas com defici√™ncia auditiva, a melhoria na indexa√ß√£o e busca de v√≠deos em plataformas digitais, o suporte √† tradu√ß√£o autom√°tica e a cria√ß√£o de audiodescri√ß√£o para pessoas com defici√™ncia visual. Al√©m disso, essa tecnologia pode ser aplicada em √°reas como vigil√¢ncia, educa√ß√£o e entretenimento, facilitando o entendimento e a an√°lise de v√≠deos de forma automatizada.

---
## Sum√°rio
* <a href="#Datasets">Datasets</a>
* <a href="#Uso">Uso</a>
* <a href="#Modelo">Modelo</a>
  * <a href="#Entrada">Entrada</a>
  * <a href="#Arquitetura">Arquitetura
  * <a href="#M√©tricas">M√©tricas</a>
* <a href="#Scripts">Scripts</a>
* <a href="#Melhorias Futuras">Melhorias Futuras</a>
* <a href="#Refer√™ncias">Refer√™ncias</a>


---
<h2 id="Datasets">Datasets</h2>

Para treinamento, teste e valida√ß√£o foram usados dois datasets reconhecidos pela literatura.

<a href="https://github.com/Soldelli/MAD">MAD</a>: MAD √© um Dataset coletado de Descri√ß√µes de √Åudio de Filmes. Compreende um total de 384 mil frases baseadas em mais de 1,2 mil horas de v√≠deos cont√≠nuos de 650 filmes diferentes e diversos. Abrangendo mais de 22 g√™neros em 90 anos de hist√≥ria do cinema, MAD cobre um amplo dom√≠nio de a√ß√µes, locais e cenas.

<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/access-to-mpii-movie-description-dataset">MPII</a>: O Dataset MPII Movie Description (MPII-MD) cont√©m um
corpus paralelo de mais de 68 mil frases e trechos de v√≠deo de 94 filmes em HD, caracterizado por meio de benchmarking de diferentes abordagens para gerar descri√ß√µes de v√≠deo.

---
<h2 id="Uso">Uso</h2>

Inicialmente √© importante instalar os pacotes necess√°rios presentes no arquivo requeriments.txt

   ```bash
   pip install -r requeriments.txt
   ```  
√â importante ressaltar que n√£o estamos autorizados a prover acesso aos datasets citados anteriormente, dessa forma a etapa de processamento e organiza√ß√£o de dados permanecer√° oculta nessa documenta√ß√£o.
Tendo os dados em sua vers√£o final, basta realizar o treinamento.

   ```bash
   python3 main.py
   ```  

Para testar o modelo treinado, basta usar o arquivo de infer√™ncia ou o streamlit para apresenta√ß√£o.

   ```bash
   streamlit run app.py
   ``` 
ou

   ```bash
   python3 inference.py
   ```  

---
<h2 id="Modelo">Modelo</h2>

O modelo √© baseado em uma arquitetura chamada S2VT (Sequence to Sequence - Video to Text), que utiliza redes neurais recorrentes (RNNs) com unidades LSTM (Long Short-Term Memory) para gerar descri√ß√µes em linguagem natural a partir de sequ√™ncias de v√≠deos. O modelo S2VT mapeia uma sequ√™ncia de frames de v√≠deo (entrada vari√°vel) para uma sequ√™ncia de palavras (sa√≠da vari√°vel).

<h3 id="Entrada">Entrada</h3>
Cada frame do v√≠deo √© processado pelo modelo CLIP-B32 para que as informa√ß√µes visuais sejam resumidas vetores num√©ricos de 512 dimens√µes.

Depois do processamento dos frames, √© necess√°rio escolher quais embeddings ser√£o entregues ao modelo, tendo em vista que existem muitos frames com informa√ß√µes redundantes. Dessa forma, o algortimo de clusteriza√ß√£o k-means foi usados para agrupar frames semelhantes, por isso foi poss√≠vel usar os clusters para escolher um total de 10 frames para treinamento, de forma que todas as informa√ß√µes da cena fossem entregues ao modelo. 

<h3 id=Arquitetura>Arquitetura</h3>

O n√∫cleo do S2VT √© uma pilha de duas camadas LSTM, como visto na figura abaixo:

![Arquitetura](img/arquitetura.jpeg)
*Figura 1: Arquitetura da Rede*


Primeira Camada LSTM (Encoder):

Processa os cada um dos embeddings de frames individualmente e codifica as informa√ß√µes temporais em uma camada latente (‚Ñéùë°). Cada frame √© representado pelas caracter√≠sticas do CLIP-B32 e introduzido sequencialmente na LSTM. Durante essa fase, o modelo n√£o gera sa√≠da, apenas codifica as entradas.

Segunda Camada LSTM (Decoder):

Ap√≥s todos os frames serem processados, a segunda LSTM gera a descri√ß√£o palavra por palavra. O decoder recebe o estado latente da primeira LSTM e um token especial de Begin-of-Sentence para iniciar a gera√ß√£o. A cada passo, a LSTM prev√™ a pr√≥xima palavra com base no estado oculto e na palavra gerada anteriormente. O processo termina quando o token de End-of-Sentence √© gerado.

---
<h2 id="Scripts">Scripts</h2>

 * **app.py** Streamlit que gera legendas para v√≠deos usando o modelo de captioning 
 * **dataset.py** Classe que carrega e prepara os dados para treinar o modelo de captioning
 * **inference.py** Realiza a infer√™ncia usando um modelo pr√©-treinado 
 * **main.py** Faz o treinamento do modelo de captioning
 * **model.py** Implementa do modelo de captioning usando a arquitetura Encoder-Decoder.
 * **utils.py** Implementa a fun√ß√£o respons√°vel por processar o batch de dados
 * **vocabulary.py** Define o vocabul√°rio usadono modelo

---
<h2 id="Melhorias Futuras">Melhorias Futuras</h2>

---
<h2 id="Refer√™ncias">Refer√™ncias</h2>
Venugopalan et al. (2015) propuseram o modelo S2VT para gera√ß√£o de descri√ß√µes de v√≠deos. Para mais detalhes, consulte o artigo:  
[Sequence to Sequence -- Video to Text](https://arxiv.org/abs/1505.00487) (arXiv:1505.00487).